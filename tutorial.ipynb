{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Bellatrex\n",
    "\n",
    "After making sure that the needed packages are installed, we can dive into the `tutorial.py` code.\n",
    "\n",
    "## Step 1: import libraries and set parameters\n",
    "\n",
    "Import the required libraries and set the parameters for the grid search, data folder paths, and other configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# set a small value for MAX_SAMPLES_EXPLAIN for quick code testing\n",
    "MAX_SAMPLES_EXPLAIN = 2 # default amount is len(X_test), which is capped at 100\n",
    "\n",
    "p_grid = {\n",
    "    \"n_trees\": [80, 100],\n",
    "    \"n_dims\": [2, None],\n",
    "    \"n_clusters\": [1, 2, 3]\n",
    "    }\n",
    "\n",
    "##########################################################################\n",
    "root_folder = os.getcwd()\n",
    "\n",
    "data_folder = os.path.join(root_folder, \"example-data\")\n",
    "\n",
    "''' choose appropriate learning task wth SETUP parameter '''\n",
    "# for this tutorial you cna choose between \"bin\", \"surv\", or \"mtr\" \n",
    "SETUP = \"surv\"\n",
    "\n",
    "VERBOSE = 4\n",
    "SET_PLOT_GUI = False\n",
    "\n",
    "'''  levels of verbosity in this script:\n",
    "    - >= 1.0: print best params, their achieved fidelity,\n",
    "              and the scoring method used to compute such performance\n",
    "    - >= 2.0: print final tree idx cluster sizes\n",
    "              and generate txt files with the extracted rule-paths (if FILE_OUT is not 'None' or 'False')\n",
    "    - >= 3.0: plot vector representation of the extracted trees (two plots: cluster memberships and prediction / loss])\n",
    "    - >= 4.0: plot trees with GUI (if SET_PLOT_GUI == True)\n",
    "    - >= 4.0: plot trees without GUI (if SET_PLOT_GUI == False)\n",
    "    - >= 5.0: print params and performance during GridSearch\n",
    "'''\n",
    "\n",
    "# running different RFs or different performance measures according to the \n",
    "# prediction scenarios. So far we have implemented the following 5 cases:\n",
    "binary_key_list = [\"bin\", \"binary\"]\n",
    "survival_key_list = [\"surv\", \"survival\"]\n",
    "multi_label_key_list = [\"multi\", \"multi-l\", \"multi-label\", \"mtc\"]\n",
    "regression_key_list = [\"regression\", \"regress\", \"regr\"]\n",
    "mt_regression_key_list = [\"multi-target\", \"multi-t\", \"mtr\"]\n",
    "\n",
    "\n",
    "names_map = { # associate to each example scenario the name of the dataset\n",
    "    'bin': 'blood',\n",
    "    'surv': 'whas500',\n",
    "    'mtr': 'scpf'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and preprocess Data\n",
    "\n",
    "Load training and testing data from the `.csv` files, split them into features (X) and targets (y), and preprocess the data by formatting the target variables according to the prediction scenarios. Instantiate the appropriate `RandomForest` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from code_scripts.utilities import output_X_y\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(data_folder, SETUP + '_tutorial_train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(data_folder, SETUP + '_tutorial_test.csv'))\n",
    "\n",
    "X_train, y_train = output_X_y(df_train, SETUP)\n",
    "X_test, y_test = output_X_y(df_test, SETUP)\n",
    "\n",
    "\n",
    "orig_n_labels = y_test.shape[1] # meaningful only in multi-output set-ups\n",
    "\n",
    "from code_scripts.utilities import format_targets\n",
    "\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "y_train, y_test = format_targets(y_train, y_test, SETUP, VERBOSE)\n",
    "\n",
    "### instantiate original R(S)F estimator, with moderate pruning (we do not want pure leaves)\n",
    "if SETUP.lower() in survival_key_list:\n",
    "    clf = RandomSurvivalForest(n_estimators=100, min_samples_split=10,\n",
    "                              random_state=0)\n",
    "\n",
    "elif SETUP.lower() in binary_key_list + multi_label_key_list:\n",
    "    clf = RandomForestClassifier(n_estimators=100, min_samples_split=5,\n",
    "                                random_state=0)\n",
    "    \n",
    "elif SETUP.lower() in regression_key_list + mt_regression_key_list:\n",
    "    clf = RandomForestRegressor(n_estimators=100, min_samples_split=5,\n",
    "                               random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Instantiate and fit the Model\n",
    "\n",
    "Once the Random Forest is instantiated, the `fit` method in Bellatrex trains the Random Forest and set the parameters for Bellatrex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_scripts.LocalMethod_class import Bellatrex\n",
    "\n",
    "# fit RF here. The hyperparameters are given      \n",
    "Bellatrex_fitted = Bellatrex(clf, SETUP,\n",
    "                            p_grid=p_grid,\n",
    "                            proj_method=\"PCA\",\n",
    "                            dissim_method=\"rules\",\n",
    "                            feature_represent=\"weighted\",\n",
    "                            n_jobs=1,\n",
    "                            verbose=VERBOSE,\n",
    "                            colormap= 'RdYlBu_r',\n",
    "                            plot_GUI=SET_PLOT_GUI).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we store train data predictions. This is useful for plotting the output distribution and compare it to the test data predictions.\n",
    "\n",
    "In this example we store the .npy array as: data_name + '_tutorial_y_train_preds.npy'\n",
    "You might want to change it according to the dataset and fold under consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_scripts.utilities import predict_helper\n",
    "y_train_pred = predict_helper(Bellatrex_fitted.clf, X_train)\n",
    "\n",
    "data_name = str(names_map[SETUP])\n",
    "\n",
    "npy_savefile = os.path.join(data_folder, data_name + '_y_train_preds.npy')\n",
    "\n",
    "np.save(npy_savefile, y_train_pred)\n",
    "print('prediction distribution saved in:', npy_savefile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Make predictions, output explanations\n",
    "\n",
    "Loop through the instances of the test set, make predictions using the Bellatrex local method, and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_scripts.utilities import score_method\n",
    "# store, for every sample in the test set, the predictions from BELLATREX and the original R(S)F for comparison\n",
    "N = min(X_test.shape[0], MAX_SAMPLES_EXPLAIN)        \n",
    "\n",
    "# store final Bellatrex predictions here.\n",
    "# y_pred will be a (n_samples, n_outputs_)-array for multi-output predictions , or (n_samples,)-array for single output predictions\n",
    "y_pred = np.empty((0, 0))\n",
    "# We harmonise the output shapes with scikit-learn's Random Forest outputs by means of the predict_helper function\n",
    "from code_scripts.utilities import predict_helper, concatenate_helper\n",
    "y_ens_pred = predict_helper(clf, X_test)\n",
    "\n",
    "j = 0 # Setting fold index = 0. No cross-validation needed in this tutorial.\n",
    "\n",
    "for i in range(N): # for every sample in the test set: call the .explain method.\n",
    "    # The hyperparameters were given in the .fit. and are actively used and tuned for every instance\n",
    "    '''\n",
    "    the .explain outputs:\n",
    "        - the local prediction \n",
    "        - information about the Bellatrex instance: optimal parameters,\n",
    "                    final extracted trees/rules, their weight in the prediction, etc... \n",
    "    '''\n",
    "    FILE_OUT = None\n",
    "    # uncomment the next 3 lines to store the explanations as .txt files :\n",
    "    FILENAME_OUT = \"Rules_\"+data_name+\"_f\"+str(j)+'_id'+str(i)+'.txt'\n",
    "    FILE_OUT = os.path.join(root_folder, 'example-explanations', FILENAME_OUT)\n",
    "    print('Extracted rules stored in', FILE_OUT)\n",
    "\n",
    "    sample_info, fig, ax = Bellatrex_fitted.explain(X_test, i, out_file=FILE_OUT) # tuning is performed within the .explain method\n",
    "    y_local_pred = sample_info.local_prediction() \n",
    "    # We concatenate BELLATREX outputs by means of the concatenate_helper function\n",
    "    y_pred = concatenate_helper(y_pred, y_local_pred, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 (extra): Enhanced explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisation import read_rules, plot_rules\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(N):\n",
    "\n",
    "    print(\"Plotting explanation for instance\", i, flush=True)\n",
    "\n",
    "    rules, preds, baselines, weights, other_preds = read_rules(\n",
    "        file       = f\"example-explanations/Rules_{data_name}_f0_id{i}.txt\",\n",
    "        file_extra = f\"example-explanations/Rules_{data_name}_f0_id{i}-extra.txt\"\n",
    "    )\n",
    "    preds_distr = np.load(f\"example-data/{data_name}_y_train_preds.npy\")\n",
    "    fig, axs = plot_rules(rules, preds, baselines, weights, \n",
    "                max_rulelen=6, other_preds=other_preds, preds_distr=preds_distr,\n",
    "    )\n",
    "    # axs[0, 0].set_xlim([-0.05, 0.6]) # manually set x-axis for density plot\n",
    "    plt.show()\n",
    "    print('#'*30)\n",
    "    # plt.savefig(\"visualisation.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btrex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
