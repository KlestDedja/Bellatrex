{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "Upload needed libraries and setting up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import bellatrex as btrex\n",
    "print(\"Bellatrex version:\", btrex.__version__)\n",
    "\n",
    "PLOT_GUI = False\n",
    "\n",
    "##########################################################################\n",
    "root_folder = os.getcwd()\n",
    "print(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from bellatrex.datasets import load_mtr_data, load_mlc_data\n",
    "from bellatrex.datasets import load_survival_data, load_binary_data, load_regression_data\n",
    "from bellatrex.utilities import get_auto_setup\n",
    "\n",
    "# X, y = load_binary_data(return_X_y=True)\n",
    "# X, y = load_regression_data(return_X_y=True)\n",
    "# X, y = load_survival_data(return_X_y=True)\n",
    "X, y = load_mlc_data(return_X_y=True)\n",
    "# X, y = load_mtr_data(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the prediction task, we train a compatible Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETUP = get_auto_setup(y) # not necessary, but comfortable while swithcing between mnay prediction tasks\n",
    "print('Detected prediction task \\'SETUP\\':', SETUP)\n",
    "\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "### instantiate original R(S)F estimator, works best with some pruning.\n",
    "if SETUP.lower() in 'survival':\n",
    "    clf = RandomSurvivalForest(n_estimators=100, min_samples_split=10,\n",
    "                                n_jobs=-2, random_state=0)\n",
    "\n",
    "elif SETUP.lower() in ['binary', 'multi-label']:\n",
    "    clf = RandomForestClassifier(n_estimators=100, min_samples_split=5,\n",
    "                                n_jobs=-2, random_state=0)\n",
    "\n",
    "elif SETUP.lower() in ['regression', 'multi-target']:\n",
    "    clf = RandomForestRegressor(n_estimators=100, min_samples_split=5,\n",
    "                                n_jobs=-2, random_state=0)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('Model fitting complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the RF model to be explained is already trained externally, it can be loaded and packed with the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bellatrex.wrapper_class import pack_trained_ensemble\n",
    "\n",
    "# Pretrained RF model should be packed as a list of dicts with the function below.\n",
    "clf_packed = pack_trained_ensemble(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `pack_trained_ensemble` stores a memory-efficient version of the model, making the model \n",
    "compatible not only with Bellatrex but also with [SHAP](https://shap.readthedocs.io/en/latest/)\n",
    "\n",
    "The packed `clf_packed` model can now be passed into Bellatrex.\n",
    "\n",
    "## Building Explanations\n",
    "Now we can fit Bellatrex on the training data and run it on a few test samples\n",
    "After fitting and tuning the explainer to a specific test instance, you can:\n",
    "- `plot_overview()` to get a representation of the tree learners, and of the selected rules;\n",
    "    GUI is available for this plotting method (set the paramter `plot_gui = True`).\n",
    "- `plot_visuals()` to visualise the selected rules in a more use friendly way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bellatrex import BellatrexExplain\n",
    "from bellatrex.utilities import predict_helper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#fit RF here. The hyperparameters for fitting the explanation are given\n",
    "# compatible with trained ensemble model clf, and with packed dictionary as in clf_packed\n",
    "Btrex_fitted = BellatrexExplain(clf_packed, set_up='auto',\n",
    "                                p_grid={\"n_clusters\": [1, 2, 3]},\n",
    "                                verbose=3).fit(X_train, y_train)\n",
    "\n",
    "N_TEST_SAMPLES = 2\n",
    "for i in range(N_TEST_SAMPLES):\n",
    "\n",
    "    print(f\"Explaining sample i={i}\")\n",
    "\n",
    "    y_train_pred = predict_helper(clf, X_train) # calls, predict or predict_proba, depending on the underlying model\n",
    "\n",
    "    tuned_method = Btrex_fitted.explain(X_test, i)\n",
    "\n",
    "    fig1, axs1 = tuned_method.plot_overview(plot_gui=False,\n",
    "                               show=True)\n",
    "    plt.show(fig1)\n",
    "\n",
    "    if SETUP.lower() in ['binary', 'regresssion', 'survival']:\n",
    "\n",
    "        fig2, axs2 = tuned_method.plot_visuals(plot_max_depth=5,\n",
    "                              preds_distr=y_train_pred,\n",
    "                              conf_level=0.9,\n",
    "                              tot_digits=4, show=False)\n",
    "        plt.show(fig2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `nbstripout` to Strip Jupyter Notebook Metadata\n",
    "\n",
    "During developing and testing, this notebook might undergo changes. To prevent metadata (e.g execution_count) from being synchronized by Git, we need to install `nbstripout`, and we therefore add it to the `dev` dependencies. \n",
    "\n",
    "This will ensure that the `pip install nbstripout` is called while pip installing `bellatrex`\n",
    "### Configuration and installation\n",
    "\n",
    "To configure behaviour we created a `.nbstripout.json` file, and placed in the root of the repo.\n",
    "The path is not always recognized, so you might need to run a bash command like the following:\n",
    "```\n",
    "/c/Users/path/to/Scripts/nbstripout.exe --install\n",
    "```\n",
    "\n",
    "This installs a pre-commit Git hook that strips notebooks automatically when committing.\n",
    "\n",
    "Finally the last difficult step consists in stripping the metadata from all notebooks. With (Git) bash:\n",
    "```\n",
    "find . -name \"*.ipynb\" -exec /c/Users/path/to/anaconda3/envs/btrex/Scripts/nbstripout.exe {} \\;\n",
    "```\n",
    "\n",
    "You can now git add and git commit as usual! `nbstripout` will take care of stripping the metadata during its pre-commit checks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btrex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
